{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68854dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JWeinstein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JWeinstein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JWeinstein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "pd.get_option(\"display.max_columns\")\n",
    "\n",
    "# download nltk features and create punctuation list\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "punctuation = list(punctuation)\n",
    "# add quote sign \"``\" and \"''\"\n",
    "punctuation.extend([\"``\",\"''\"])\n",
    "\n",
    "\n",
    "# function to convert a webpage in txt format into a sequence of stemmed tokens, stored as list in a dictionary\n",
    "\n",
    "def load_wikipages(directory):\n",
    "    \"\"\"\n",
    "    load webpages from a given directoy path and tokenize+stem each page and stores it in a dicionary.\n",
    "    - input: directory\n",
    "    - returns: dictionary containing the tokenized webpages\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # initialize dictionary\n",
    "    docs = {}\n",
    "    doc_id=1\n",
    "    # iterate over files in that directory\n",
    "    for filename in sorted(os.listdir(directory), key=lambda x: int(x.split(')')[0])):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            with open(f, 'r', encoding='utf-8') as d:\n",
    "                url = d.readline().strip()\n",
    "                title = d.readline().strip()\n",
    "                doc = d.read()\n",
    "               \n",
    "            word_tokens = nltk.word_tokenize(doc)\n",
    "            tokens_swremoved = [w for w in word_tokens if w.lower() not in stop_words]\n",
    "            tokens_stemmed = [stemmer.stem(w) for w in tokens_swremoved]\n",
    "            tokens_puncremoved = [token for token in tokens_stemmed if token not in punctuation]\n",
    "            docs[(f'd{doc_id}',url, title)] = tokens_puncremoved   \n",
    "            doc_id += 1\n",
    "    \n",
    "    return docs\n",
    "            \n",
    "directory = \"C:/Users/JWeinstein/Capstone-main/src/Raw_TXT_Downloads/\"\n",
    "docs = load_wikipages(directory)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af158fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the inverted index\n",
    "\n",
    "def build_inverted_index(docs, min_df=1):\n",
    "    \"\"\"\n",
    "    builds an inverted index for a collection of given webpages\n",
    "    - input: docs in dictionary format\n",
    "    - returns: inverted index as dictionary, where the keys are the words from the webpages, and the values \n",
    "        are the mapping of the documents to the word. The values are again a dictionary containing the doc ID \n",
    "        as keys, and the values are lists in the format e.g. [2, [3, 5]], which means the word from the key of the\n",
    "        outer dictionary occurs 2 times at position 3 and 5.\n",
    "    \"\"\"\n",
    "\n",
    "    inv_index = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for doc in docs:\n",
    "        posting = defaultdict(list)\n",
    "        for t_idx, t in enumerate(docs[doc]):\n",
    "            posting[t].append(t_idx)\n",
    "\n",
    "        for t in posting:\n",
    "            inv_index[t][doc].extend([len(posting[t]), posting[t]])\n",
    "    \n",
    "    # filter out tokens which appear less than min_df\n",
    "    inv_index = {token: dict(inv_index[token]) for token in inv_index if len(inv_index[token]) >= min_df}\n",
    "    \n",
    "    return inv_index\n",
    "\n",
    "inv_index = build_inverted_index(docs)\n",
    "\n",
    "len(inv_index)\n",
    "\n",
    "\"\"\"Store inverted index as pickle file\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save dictionary to pickle file\n",
    "with open('C:/Users/JWeinstein/Capstone-main/src/inv_index.pickle', 'wb') as file:\n",
    "    pickle.dump(inv_index, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2664ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "nltk                3.7\n",
       "numpy               1.21.5\n",
       "pandas              1.4.2\n",
       "session_info        1.0.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "asttokens                   NA\n",
       "backcall                    0.2.0\n",
       "beta_ufunc                  NA\n",
       "binom_ufunc                 NA\n",
       "bottleneck                  1.3.4\n",
       "colorama                    0.4.4\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "executing                   0.8.3\n",
       "google                      NA\n",
       "importlib_metadata          NA\n",
       "ipykernel                   6.9.1\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.1\n",
       "joblib                      1.1.0\n",
       "jupyter_server              1.13.5\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "nbinom_ufunc                NA\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numexpr                     2.8.1\n",
       "packaging                   21.3\n",
       "parso                       0.8.3\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prompt_toolkit              3.0.20\n",
       "psutil                      5.8.0\n",
       "pure_eval                   0.2.2\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pythoncom                   NA\n",
       "pytz                        2021.3\n",
       "pywintypes                  NA\n",
       "regex                       2.5.112\n",
       "scipy                       1.7.3\n",
       "setuptools                  65.4.0\n",
       "six                         1.16.0\n",
       "sklearn                     1.0.2\n",
       "sphinxcontrib               NA\n",
       "stack_data                  0.2.0\n",
       "threadpoolctl               2.2.0\n",
       "tornado                     6.1\n",
       "traitlets                   5.1.1\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32security               NA\n",
       "zipp                        NA\n",
       "zmq                         22.3.0\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.2.0\n",
       "jupyter_client      6.1.12\n",
       "jupyter_core        4.9.2\n",
       "jupyterlab          3.3.2\n",
       "notebook            6.4.8\n",
       "-----\n",
       "Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.19044-SP0\n",
       "-----\n",
       "Session information updated at 2022-12-05 14:52\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
