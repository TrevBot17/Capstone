{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# function to transform a given query into a list of tokenized+stemmed words\n",
    "def query_prep(query):\n",
    "    \"\"\"convert a given query into a list of tokenized and stemmed words\"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(query)\n",
    "    tokens_swremoved = [w for w in word_tokens if w.lower() not in stop_words]\n",
    "    tokens_stemmed = [stemmer.stem(w) for w in tokens_swremoved]\n",
    "\n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "# OkapiBM25 retrieval function\n",
    "def OkapiBM25(inverted_index, queries, max_docs=-1, k1=1.2, b=0.75, k3=1000):\n",
    "    \"\"\"\n",
    "    Retrieve webpages in order of relevance from an inverted index based on a query. The function looks only \n",
    "    at terms of the query which are present in the inverted index. The ranking is based on the retrieval function \n",
    "    S(D, Q)=sum(((k1+1)*c_wD)/(k1*(1-b+b*(|D|/avg_dl)+c_wD)) * ln((N-df(w)+0.5)/(df(w)+0.5) * ((k3+1)*c_wQ)/(k3+c_wQ)). \n",
    "    \n",
    "    - input: inverted index, query, max_docs (max number of docs to be retrieved), and parameters k1, b, k3\n",
    "    - returns the webpages in descending order of relevance\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ret_docs = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # dict for storing the document length\n",
    "    doc_dict = {}\n",
    "    for token, posting in inverted_index.items():\n",
    "        for doc, val in posting.items():\n",
    "            if doc not in doc_dict:\n",
    "                doc_dict[doc] = val[0]\n",
    "            else:\n",
    "                doc_dict[doc] += val[0]\n",
    "    # average doc length\n",
    "    avg_dl = np.mean([value for key, value in doc_dict.items()]) \n",
    "\n",
    "    ret_docs= {}            \n",
    "\n",
    "    for query in queries:\n",
    "        # dict for each query (temporary)\n",
    "        docs = {}\n",
    "        for token in queries[query]:\n",
    "            # calculate query term frequency c_wQ\n",
    "            c_wQ = queries[query].count(token)\n",
    "            # check if token is in index\n",
    "            if token in inverted_index.keys():\n",
    "                # IDF: formula: ln((N-df(w)+0.5)/(df(w)+0.5), Note: IDF is not dependent on a particular document!\n",
    "                idf = math.log((len(doc_dict)-len(inverted_index[token])+0.5)/(len(inverted_index[token])+0.5))\n",
    "                # traverse through the documents in the inverted index for the given token, calculate the score per token \n",
    "                # using Okapi/BM25 and store the per-document accumulated version of it in dict docs\n",
    "                # (dictionary accumulating pattern)\n",
    "                for doc in inverted_index[token]:\n",
    "                    # TF=((k1+1)*c_wD)/(k1*(1-b+b*(|D|/avg_dl)+c_wD))\n",
    "                    c_wD = inverted_index[token][doc][0]\n",
    "                    tf = ((k1 + 1) * c_wD) / (k1 * (1 - b + b * (doc_dict[doc]/avg_dl)) + c_wD)\n",
    "                    # normalized query term frequency: qtf=((k3+1)*c_wQ)/(k3+c_wQ)\n",
    "                    qtf = ((k3 + 1) * c_wQ) / (k3 + c_wQ)\n",
    "                    score = tf * idf * qtf\n",
    "                    if doc not in docs:\n",
    "                        docs[doc] = score\n",
    "                    else:\n",
    "                        docs[doc] += score\n",
    "                        \n",
    "        # rounding before sorting\n",
    "        docs = {k: round(v,3) for k, v in docs.items()}\n",
    "\n",
    "        # options for max_docs\n",
    "        if max_docs == -1:\n",
    "            docs_per_query = sorted(Counter(docs).most_common(), key=lambda x: (-x[1], int(x[0][0].split('d')[1])))\n",
    "        else:\n",
    "            docs_per_query = sorted(Counter(docs).most_common(), key=lambda x: (-x[1], int(x[0][0].split('d')[1])))[:max_docs]\n",
    "        # grab the doc id out of docs_per_query\n",
    "        ret_docs[query] = [doc for doc, freq in docs_per_query]\n",
    "                \n",
    "    return ret_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
